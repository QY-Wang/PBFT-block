package eth

// Copyright 2015 The go-ethereum Authors
// This file is part of the go-ethereum library.
//
// The go-ethereum library is free software: you can redistribute it and/or modify
// it under the terms of the GNU Lesser General Public License as published by
// the Free Software Foundation, either version 3 of the License, or
// (at your option) any later version.
//
// The go-ethereum library is distributed in the hope that it will be useful,
// but WITHOUT ANY WARRANTY; without even the implied warranty of
// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
// GNU Lesser General Public License for more details.
//
// You should have received a copy of the GNU Lesser General Public License
// along with the go-ethereum library. If not, see <http://www.gnu.org/licenses/>.

import (
	"fmt"
	//"github.com/DWBC-ConPeer/pns"

	//"bytes"
	//"fmt"
	"math/big"
	"sync"
	"sync/atomic"
	"time"

	//"github.com/DWBC-ConPeer/accounts"
	"github.com/DWBC-ConPeer/common"
	//"github.com/DWBC-ConPeer/core"

	//"github.com/DWBC-ConPeer/core/AccContract"
	//"github.com/DWBC-ConPeer/DWVM"
	//"github.com/DWBC-ConPeer/pns"

	//"github.com/DWBC-ConPeer/core/state"
	//"github.com/DWBC-ConPeer/core/types"
	//"github.com/DWBC-ConPeer/core/types/BlockDM"
	//"github.com/DWBC-ConPeer/core/types/HeaderDM"
	//"github.com/DWBC-ConPeer/core/types/ReceiptDM"
	//"github.com/DWBC-ConPeer/core/types/TXDM"
	//"github.com/DWBC-ConPeer/core/vm"
	//"github.com/DWBC-ConPeer/ethdb"
	"github.com/DWBC-ConPeer/event"
	"github.com/DWBC-ConPeer/logger"
	"github.com/DWBC-ConPeer/logger/glog"
	//"github.com/DWBC-ConPeer/node"
	//"github.com/DWBC-ConPeer/params"
	//"github.com/DWBC-ConPeer/pow"
)

type ChainConfig struct {
	String string
}

type StateDB struct {
	String string
}
type PoW struct {
	String string
}
type Set struct {
	String string
}

type Transactions struct {
	String string
}

type Transaction struct {
	String string
}

type Receipt struct {
	String string
}

type Backend struct {
	String string
}

func (self Backend) TxPool() ([51]int, [51]int) {
	var objects_ExeSeqs [51]int
	var dependency_TX [51]int
	//dependency_TXå’Œobject_ExeSeqsåˆ†åˆ«ä¸ºäº¤æ˜“åºåˆ—å’Œäº¤æ˜“ä¾èµ–ï¼Œè¿™é‡Œæš‚æ—¶ä½¿ç”¨ä¸¤ä¸ªæ•°ç»„è¡¨ç¤º
	for i := 0; i < 51; i++ {
		objects_ExeSeqs[i] = i + 1
	}
	for i := 0; i < 51; i++ {
		dependency_TX[i] = 50 - i
	}
	return objects_ExeSeqs, dependency_TX
}

type Database struct {
	String string
}

type Validator struct {
	String string
}

type DWCCVM struct {
	String string
}

//ä¸Šé¢æ˜¯æ”¹çš„
//var MinerWaitGroup = new(sync.WaitGroup)

var jsonlogger = logger.NewJsonLogger()

const (
	resultQueueSize  = 10
	miningLogAtDepth = 5
)

// Agent can register themself with the worker
type Agent interface {
	Work() chan<- *Work
	SetReturnCh(chan<- *Result)
	Stop()
	Start()
	GetHashRate() int64
}

type uint64RingBuffer struct {
	ints []uint64 //array of all integers in buffer
	next int      //where is the next insertion? assert 0 <= next < len(ints)
}

// environment is the workers current environment and holds
// all of the current state information
type Work struct {
	config           *ChainConfig
	state            *StateDB // apply state changes here
	ancestors        *Set     // ancestor set (used for checking uncle parent validity)
	family           *Set     // family set (used for checking uncle invalidity)
	uncles           *Set     // uncle set
	tcount           int      // tx count in cycle
	ownedAccounts    *Set
	lowGasTxs        Transactions
	failedTxs        Transactions
	localMinedBlocks *uint64RingBuffer // the most recent block numbers that were mined locally (used to check block inclusion)

	Block *Block // the new block

	header   *Header
	txs      []*Transaction
	receipts []*Receipt

	createdAt time.Time
}

type Result struct {
	Work  *Work
	Block *Block
}

//æ”¹ï¼š
type BlockChain struct {
	CurrentBlock uint64 //å½“å‰chainçš„é«˜åº¦
}

func (self *BlockChain) ReturnCurrentBlock() *Block {
	return &currentblock
}

type Logs struct {
}
type Node struct {
}
type ConsensuReponse struct {
	Timestamp time.Time
	Sender    string
	View      uint64
	LeaderID  string
}
type NewLeaderSet struct {
	Sender    string
	View      uint64
	Round     uint64
	Leader    string
	Votes     []*VoteForLeader
	BH        uint64
	Timestamp time.Time
}
type VoteForLeader struct {
	Sender    string
	VoteFor   string
	View      uint64
	BH        uint64
	Round     uint64
	Timestamp time.Time
	Sign      string
}

func (self *worker) SetReady(flag bool) {
	self.rmux.Lock()
	defer self.rmux.Unlock()
	self.ready = flag
}

/*
	æ‰©å……workerï¼Œä»¥é€‚åº”pbft
*/
// worker is the main object which takes care of applying messages to the new state
type worker struct {
	txNum_startconsens int //ä»…ç”¨äºæµ‹è¯•ï¼Œæ„å»ºåŒ…å«ä¸€å®šæ•°é‡çš„äº¤æ˜“çš„åŒºå—
	config             *ChainConfig

	mu sync.Mutex

	// update loop
	mux    *event.TypeMux
	events event.Subscription
	wg     sync.WaitGroup

	agents map[Agent]struct{}
	recv   chan *Result
	pow    PoW

	eth Backend // eth chain å¯ä»¥è¿›è¡Œblockchainæ“ä½œ
	//æ”¹ï¼š chain *core.BlockChain
	chain *BlockChain

	proc Validator
	//chainDb ethdb.Database
	dbs map[string]Database

	coinbase common.Address // åç»­ç”¨åˆ°
	gasPrice *big.Int
	extra    []byte

	currentMu sync.Mutex
	current   *Work

	uncleMu        sync.Mutex
	possibleUncles sync.Map

	txQueueMu sync.Mutex
	txQueue   sync.Map

	// atomic status counters
	mining int32
	atWork int32

	fullValidation bool
	//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~//

	activeView bool   // view change happening
	view       uint64 // current view

	// implementation of PBFT `in`
	reqStore        sync.Map           // map[string]*core.Request    track requests
	certStore       map[msgID]*msgCert //track quorum certificates for requests
	certMux         sync.RWMutex       // cert çš„è¯»å†™é”
	checkpointStore sync.Map           // map[vcidx]*core.Checkpoint //track checkpoints as set
	viewChangeStore sync.Map           // map[uint64]map[string]*core.ViewChange  view sender viewchange
	newViewStore    sync.Map           // map[uint64]*core.NewView track last new-view we received or sent

	commitStore sync.Map //å­˜å‚¨ç­¾è¿‡åçš„åŒºå—

	Stack   *Node      // node.Node
	stackMu sync.Mutex // stack çš„äº’æ–¥é”

	//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~//

	PbftMux    *event.TypeMux
	pbftevents event.Subscription

	N int // max.number of validators in the network. replica count

	isStartSelfCheck bool //æ˜¯å¦å¼€å¯ä¸»èŠ‚ç‚¹è‡ªæ£€å‡½æ•°
	selfcheckmux     sync.RWMutex

	quroummux sync.RWMutex // N,id and son on

	viewmux sync.RWMutex // activeView lock//FIXME è¿™ä¸ªæ˜¯å¦è¿˜ä¿ç•™ï¼Ÿä»¥å‰è¢«åˆ äº†

	conviewmux sync.RWMutex // view lock

	conPNode sync.Map //map[string]string // è®¸å¯çš„å…±è¯†èŠ‚ç‚¹åˆ—è¡¨ï¼ŒK:nodeidï¼ŒVï¼šnodeidçš„sortid

	nodeID string       // æœ¬èŠ‚ç‚¹çš„nodeid
	nidmux sync.RWMutex // nodeid lock

	//FIXME æ”¹æˆå¹¶å‘map  å»é”
	crcertstore       map[uint64]map[string]ConsensuReponse // consensus reponse certstore, format: view:sender:cr
	crcertmux         sync.RWMutex                          // cert çš„è¯»å†™é”
	newLeaderSetStore map[string]NewLeaderSet               ///sender/è®¾ç½®äº†æ–°çš„leader
	nlStoreMux        sync.RWMutex
	//å­˜å‚¨è½¬å‘çš„é—®é¢˜

	leader    string // id of leader , use conviewmux lock
	preLeader string //use for viewchange
	votesFor  string // node votes info, use conviewmux lock

	votescert sync.Map //map[uint64]map[string]core.VoteForLeader // store the votes info

	ready bool // consensus flag
	rmux  sync.RWMutex

	electionflag bool
	electionmux  sync.RWMutex

	blockbeats     map[string]time.Time // è®°å½•åŒºå—è¢«å…±è¯†çš„æ—¶é—´
	pvbAndBeatsMux sync.RWMutex

	PNService *PermissionedNodeService

	/*
	* ä»¥åŒºå—ä¸ºå¯¹è±¡çš„å…±è¯†
	 */
	// possibleValidBlock å¯èƒ½æ­£ç¡®çš„blockï¼Œç¬¬ä¸€ä¸ªkeyè¡¨ç¤ºåŒºå—é«˜åº¦ï¼Œç¬¬äºŒä¸ªkeyè¡¨ç¤ºblockçš„hashï¼Œç¬¬äºŒä¸ªvalueè¡¨ç¤ºå…·ä½“çš„block
	possibleValidBlock map[uint64]map[string]*Block

	pendingstate *BlockChain // ä¸€ä¸ªå®Œå…¨çš„å‰¯æœ¬ï¼Œä¸€ä¸ªä¸´æ—¶çŠ¶æ€æ•°æ®åº“ï¼Œç”¨äºblockçš„åˆ›å»ºä¸éªŒè¯; è®°å¾—éªŒè¯æˆ–åˆ›å»ºåŒºå—ä¹‹åè¿›è¡Œè¿˜åŸï¼ŒéªŒè¯é€šè¿‡åï¼Œåœ¨çœŸçš„blockchainå†™å…¥

	LastReqID    uint64 // requestçš„è¯·æ±‚æ ‡å·ï¼Œåˆå§‹åŒ–ä¸ºå½“å‰åŒºå—é«˜åº¦ï¼Œè‹¥æ¥æ”¶åˆ°æ¯”æœ¬åœ°èŠ‚ç‚¹å¤§çš„numberï¼Œåˆ™æ›´æ–°ä¸ºå¤§çš„å€¼
	hasUnDealTxs bool   // ç”¨äºåˆ¤æ–­åœ¨å…±è¯†æœŸé—´æ˜¯å¦æœ‰äº¤æ˜“å…¥æ± ï¼ŒFIMXE æœªåŠ é”

	ccvm      *DWCCVM
	chainName string
	chkpt     bool //æ ‡å¿—æ­£åœ¨è¿›è¡Œcheckpoint
	chkptMux  sync.RWMutex
	initView  uint64 //è®°å½•å¯åŠ¨æ—¶çš„viewï¼Œç”¨äºåœ¨åé¢åˆ¤æ–­èŠ‚ç‚¹å½“å‰çŠ¶æ€æ˜¯ä¸æ˜¯åˆšå¯åŠ¨çŠ¶æ€ï¼Œæ›¾ç»ä¸­æ–­è¿‡ç­‰
}

type msgID struct { // our index through certStore
	v uint64 // view
	n uint64 // reqid
}

type msgCert struct {
	prePrepare  *PrePrepare
	sentPrepare bool
	prepare     []Prepare
	sentCommit  bool
	commit      []Commit

	IsPrepre bool // whether exit  prepred
	IsPre    bool // whether exit  prepared
	IsComitt bool // whether exit  comitted
}

//var err error

//func lenOfSyncMap(val sync.Map) int {
//	len := 0
//	val.Range(func(k, v interface{}) bool {
//		len++
//		return true
//	})
//	return len
//}

// add a new param stack node.Node

/*
func newWorker(config *ChainConfig, coinbase common.Address, eth Backend, nodestack *node.Node, pbftmux *event.TypeMux, pnService *pns.PermissionedNodeService, chainName string) *worker {
	checkpointBH = eth.BlockChain().CurrentBlock().NumberU64()
	checkpointStore.Store(eth.BlockChain().CurrentBlock().Hash(), checkpointBH)
	worker := &worker{
		config: config,
		eth:    eth,
		mux:    eth.EventMux(), // å…¨å±€äº‹ä»¶å¤šè·¯å¤ç”¨
		//chainDb:        eth.ChainDb(),
		dbs:            eth.Dbs(),
		recv:           make(chan *Result, resultQueueSize),
		gasPrice:       new(big.Int),
		chain:          eth.BlockChain(),
		proc:           eth.BlockChain().Validator(),
		coinbase:       coinbase,
		agents:         make(map[Agent]struct{}),
		fullValidation: false,

		PbftMux: pbftmux,
		//minermux: minermux, //has delete
		blockbeats:         make(map[string]time.Time),
		certStore:          make(map[msgID]*msgCert),
		checkpointStore:    sync.Map{},
		crcertstore:        make(map[uint64]map[string]ConsensuReponse),
		newViewStore:       sync.Map{},
		possibleValidBlock: make(map[uint64]map[string]*Block),
		reqStore:           sync.Map{},
		txQueue:            sync.Map{},
		viewChangeStore:    sync.Map{},
		votescert:          sync.Map{},

		pendingstate:      eth.BlockChain().Copy(), // need add Copy function in Blockchain
		PNService:         pnService,
		LastReqID:         eth.BlockChain().CurrentBlock().Number().Uint64(),
		chainName:         chainName,
		newLeaderSetStore: make(map[string]core.NewLeaderSet),
	}
	var pUncles sync.Map
	worker.possibleUncles = pUncles

	worker.view = eth.BlockChain().CurrentBlock().NumberU64() / TermNum // view åˆå§‹åŒ–
	worker.initView = worker.view
	worker.Stack = nodestack // è·å–nodeï¼ˆæœ¬åœ°èŠ‚ç‚¹ï¼‰çš„ç›¸å…³ä¿¡æ¯
	// TODO conPNode æ”¹ä¸ºä»çŠ¶æ€æ ‘çš„SystemConfigä¸­è·å–
	worker.conPNode = pnService.GetPermissionedConNodes() // è·å–æ‰€æœ‰è®¸å¯çš„å…±è¯†èŠ‚ç‚¹
	worker.nodeID = nodestack.GetLocalNodeID()            // è·å–æœ¬èŠ‚ç‚¹çš„nodeid

	worker.N = lenOfSyncMap(worker.conPNode) // å…±è¯†èŠ‚ç‚¹æ•°é‡

	worker.activeView = true // é»˜è®¤ä¸ºtrue
	worker.chkpt = false     //æ ‡å¿—æ­£åœ¨è¿›è¡Œcheckpoint
	worker.events = worker.mux.Subscribe(core.Checkpoint{}, core.ChainHeadEvent{}, core.ChainSideEvent{}, core.TxPreEvent{}, core.NoValidateBlock{}, core.NodeTypeChange{}, core.BeNewLeader{}, core.BlockResume{})
	worker.pbftevents = worker.PbftMux.Subscribe(core.Request{}, core.PrePrepare{}, core.Prepare{}, core.Commit{}, core.ViewChange{}, core.Checkpoint{}, core.NewView{}, core.ChainEventWorker{},
		core.PeerDisCon{}, core.PeerCon{},
		core.ConsensusRequest{}, core.ConsensuReponse{}, core.SelectionLeader{},
		core.VoteForLeader{}, core.LeaderHeartbeat{}, core.PingLeader{}, core.NewLeaderSet{},
		core.ConStatusRequest{})
	//worker.closed = make(chan bool) //FIXME ä»¥å‰æœ‰ï¼Œè¢«åˆ äº†ï¼Œè¿˜è¦å—

	go worker.update()
	go worker.updatePbft()

	if worker.JudgePermissioned(worker.nodeID) {
		go worker.expirationLoop() // constatus expiration detection //FIXME
	}
	go worker.CheckConStatus() // update constatus

	return worker
}

*/
func (self *worker) setEtherbase(addr common.Address) {
	self.mu.Lock()
	defer self.mu.Unlock()
	self.coinbase = addr
}

func (self *worker) setChkpt(flag bool) {
	self.chkptMux.Lock()
	self.chkpt = flag
	self.chkptMux.Unlock()
}

func (self *worker) getChkpt() (flag bool) {
	self.chkptMux.RLock()
	defer self.chkptMux.RUnlock()
	flag = self.chkpt
	return
}

// TODO æ”¹ä¸ºä»possibleValidBlocksè·å–æ•°æ®
func (self *worker) pending() (*Block, *StateDB) {
	self.currentMu.Lock()
	defer self.currentMu.Unlock()

	var a = new(Block)
	if atomic.LoadInt32(&self.mining) == 0 {
		a.Header = self.current.header
		a.Txs = self.current.txs
		a.Receipt = self.current.receipts
		return a, self.current.state
		//return BlockDM.NewBlock(
		//	self.current.header,
		//	self.current.txs,
		//	nil,
		//	self.current.receipts,
		//), self.current.state
	}
	return self.current.Block, self.current.state //.Copy()
}

// TODO checkpoint è§¦å‘æ—¥å¿—æ¸…é™¤ç­‰åŠŸèƒ½

/*
func (self *worker) updatePbft() {
	for event := range self.pbftevents.Chan() {
		// A real event arrived, process interesting content
		switch ev := event.Data.(type) {
		case core.Request:
			if ev.Sender == self.GetSelfNodeID() {
				glog.V(logger.Info).Infof("recv request from self reqid %d, bn %d", ev.ReqID, ev.BlockHeight)
			} else {
				glog.V(logger.Info).Infof("recv request from %v, reqid %d, bn %d", ev.Sender[:len(ev.Sender)/16], ev.ReqID, ev.BlockHeight)
				go self.RecvRequest(&ev)
			}
		case core.PrePrepare:
			if self.JudgePermissioned(ev.Sender) {
				go self.RecvPrePrepare(&ev)
			}

		case core.Prepare:
			if self.JudgePermissioned(ev.Sender) {
				go self.RecvPrepare(&ev)
			}

		case core.Commit:
			if self.JudgePermissioned(ev.Sender) {
				go self.RecvCommit(&ev)
			}

		case core.ViewChange:
			if self.JudgePermissioned(ev.Sender) {
				go self.RecvViewchange(&ev)
			}
		case core.NewLeaderSet:
			if self.JudgePermissioned(ev.Sender) {
				go self.RecvNewLeaderSet(&ev)
			}
		case core.ChainEventWorker:

		case core.PeerDisCon: // åˆ¤æ–­æ˜¯å¦æ˜¯ä¸»èŠ‚ç‚¹å¤±è”äº†
			go self.MonitorLeader(&ev)

		case core.PeerCon:

		case core.ConsensusRequest:
			go self.RecvConsensusRequest(&ev)

		case core.ConsensuReponse:
			go self.RecvConsensuReponse(&ev)

		case core.ConStatusRequest:
			go self.RecvConStatusRequest(&ev)

		case core.SelectionLeader:
			go self.RecvSelectionLeader(&ev)

		case core.VoteForLeader:
			go self.RecvVotes(&ev)

		//case core.LeaderHeartbeat: //ç›®å‰ä¸å‘é€è¯¥æ¶ˆæ¯
		//	go self.RecvLeaderHeartbeat(&ev)

		default:
			glog.V(logger.Info).Infof("recv a unscripriton event %d", ev)

		}
	}
}*/

/*
func (self *worker) update() {
	for event := range self.events.Chan() {
		// A real event arrived, process interesting content
		switch ev := event.Data.(type) {
		case core.NodeTypeChange: // èŠ‚ç‚¹ç±»å‹æ›´æ–°
			glog.V(logger.Info).Infoln("update nodetype ... ")
			self.Stack.FindAndChangeConnByNodeID(ev.ID, ev.NodeType)

		case core.Checkpoint:
			if self.JudgePermissioned(ev.Sender) { //é˜²æ­¢äº¤æ˜“èŠ‚ç‚¹æ”¶åˆ°chainheadEvent æ‰§è¡Œ checkpoint
				chkptGroup.Wait()
				chkptGroup.Add(1)
				self.setChkpt(true)
				go self.RecvCheckpoint(&ev)
			}
		case core.ChainHeadEvent:
			if self.PNService.ReloadPNodes(ev.Block.NumberU64()) {
				self.conPNode = self.PNService.GetPermissionedConNodes() // è·å–æ‰€æœ‰è®¸å¯çš„å…±è¯†èŠ‚ç‚¹
				self.N = lenOfSyncMap(self.conPNode)                     // å…±è¯†èŠ‚ç‚¹æ•°é‡
			}
			go self.mux.Post(core.Checkpoint{self.GetSelfNodeID(), ev.Block.NumberU64(), 0, ev.Block.Hash()})
		case core.NoValidateBlock:
			go self.SendViewchange(ev.BlockNumber) //FIXME å–æ¶ˆæ¯ä¸­çš„ï¼Œè¿˜æ˜¯å½“å‰é«˜åº¦

		case core.TxPreEvent:
			// glog.V(logger.Info).Infoln("......  ....  revc TxPreEvent")
			if self.JudgePermissioned(self.GetSelfNodeID()) { //é˜²æ­¢äº¤æ˜“èŠ‚ç‚¹å¼€å¯å…±è¯†
				go self.StartConsensus() // å¼€å¯å…±è¯†
			}
		case core.BlockResume: // äº¤æ˜“æ± åˆ é™¤äº¤æ˜“å¹¶æ›´æ–°å®Œæˆ
			if self.JudgePermissioned(self.GetSelfNodeID()) && self.ISLeader() { //é˜²æ­¢äº¤æ˜“èŠ‚ç‚¹å¼€å¯å…±è¯†
				go self.randomwait_StartConsensus(100)
			}
		case core.BeNewLeader: // å½“èŠ‚ç‚¹æˆä¸ºä¸»èŠ‚ç‚¹åï¼Œéœ€è¦ç»™è‡ªå·±å‘é€ä¸€æ¡æ¶ˆæ¯
			// TODO  SetConStatus ä¸­åˆ¤æ–­leaderæ˜¯å¦æ˜¯æœ¬èŠ‚ç‚¹ï¼Œè‹¥æ˜¯å‘é€BeNewLeaderæ¶ˆæ¯
			//glog.V(logger.Info).Infoln("=====be new leader")
			self.ResetConData2()
			go self.randomwait_StartConsensus(250) // å¼€å¯å…±è¯†

		}
	}
}*/

//æ”¹ï¼š
func (self *worker) randomwait_StartConsensus(num int) {

	if true /*self.eth.TxPool().ExistsTxs()*/ {
		t1 := time.NewTimer(time.Microsecond * time.Duration(num))
		defer t1.Stop()
		//for {
		select {
		case <-t1.C:
			if true /*self.eth.TxPool().ExistsTxs()*/ {
				go self.StartConsensus() // å¼€å¯å…±è¯†
			}
			//goto anthor
		}
	}
	//anthor:
	//glog.V(logger.Info).Infoln("nothing do")
	//}
}

func newLocalMinedBlock(blockNumber uint64, prevMinedBlocks *uint64RingBuffer) (minedBlocks *uint64RingBuffer) {
	if prevMinedBlocks == nil {
		minedBlocks = &uint64RingBuffer{next: 0, ints: make([]uint64, miningLogAtDepth+1)}
	} else {
		minedBlocks = prevMinedBlocks
	}

	minedBlocks.ints[minedBlocks.next] = blockNumber
	minedBlocks.next = (minedBlocks.next + 1) % len(minedBlocks.ints)
	return minedBlocks
}

// makeCurrent creates a new environment for the current cycle.
func (self *worker) makeCurrent(parent *Block, header *Header) error {
	//state, err := self.pendingstate.State()
	////state, err := self.chain.StateAt(parent.Root())
	//if err != nil {
	//	glog.V(logger.Info).Infoln("self chain stateAt parent root %d, errr %d.", parent.Root(), err)
	//	return err
	//}
	state := StateDB{}
	set := Set{}
	work := &Work{
		config:    self.config,
		state:     &state,
		ancestors: &set,
		family:    &set,
		uncles:    &set,
		header:    header,
		createdAt: time.Now(),
		//eth:       self.eth,
	}

	// when 08 is processed ancestors contain 07 (quick block)
	//for _, ancestor := range self.chain.GetBlocksFromHash(parent.Hash(), 7) {

	/*æ”¹ï¼Œancestorsç­‰éƒ½è®¾ä¸ºç©ºäº†ï¼Œ
	for _, ancestor := range self.pendingstate.GetBlocksFromHash(parent.Hash(), 7) {
		for _, uncle := range ancestor.Uncles() {
			work.family.Add(uncle.Hash())
		}
		work.family.Add(ancestor.Hash())
		work.ancestors.Add(ancestor.Hash())
	}*/

	//accounts := self.eth.AccountManager().Accounts()

	// Keep track of transactions which return errors so they can be removed
	work.tcount = 0
	//work.ownedAccounts = accountAddressesSet(accounts)
	if self.current != nil {
		work.localMinedBlocks = self.current.localMinedBlocks
	}
	self.current = work
	return nil
}

func (self *worker) isBlockLocallyMined(current *Work, deepBlockNum uint64) bool {
	//Did this instance mine a block at {deepBlockNum} ?
	var isLocal = false
	for idx, blockNum := range current.localMinedBlocks.ints {
		if deepBlockNum == blockNum {
			isLocal = true
			current.localMinedBlocks.ints[idx] = 0 //prevent showing duplicate logs
			break
		}
	}
	//Short-circuit on false, because the previous and following tests must both be true
	if !isLocal {
		return false
	}

	//Does the block at {deepBlockNum} send earnings to my coinbase?
	var block = self.pendingstate.ReturnCurrentBlock() //.GetBlockByNumber(deepBlockNum)
	//var block = self.chain.GetBlockByNumber(deepBlockNum)
	return block != nil // && block./*.Coinbase()*/ == self.coinbase
}

func (self *worker) logLocalMinedBlocks(current, previous *Work) {
	//	glog.V(logger.Info).Infof(" for zhiqian nextBlockNum=%d", current.Block.NumberU64())

	if previous != nil && current.localMinedBlocks != nil {
		nextBlockNum := current.Block.Number //.NumberU64()
		for checkBlockNum := previous.Block.Number; /*.NumberU64()*/ checkBlockNum < nextBlockNum; checkBlockNum++ {
			inspectBlockNum := checkBlockNum - miningLogAtDepth
			if self.isBlockLocallyMined(current, inspectBlockNum) {
				glog.V(logger.Info).Infof("ğŸ”¨ ğŸ”—  Mined %d blocks back: block #%v", miningLogAtDepth, inspectBlockNum)
			}
		}

	}

}

// TODO å°†commitTransactionsä¸­æœ‰å…³äº¤æ˜“æ‰§è¡Œä¸éªŒè¯çš„åœ°æ–¹ï¼Œé‡‡ç”¨æ–°çš„æ¥å£ä¸å‡½æ•°ï¼Œå³ApplyTransactionç­‰
/*
func (env *Work) commitTransactions(mux *event.TypeMux, txs *TXDM.TransactionsByTypeAndNonce, bc *core.BlockChain) {
	//gp := new(core.GasPool).AddGas(env.header.GetGasLimit())

	var coalescedLogs vm.Logs

	for {
		// Retrieve the next transaction and abort if all done
		tx := txs.Peek()
		if tx == nil {
			break
		}

		// judge tx if has committed or has wrrotten into chain
		isRepeat := false
		for _, ele := range env.txs {
			if tx.Hash() == ele.Hash() {
				isRepeat = true
				break
			}
		}

		if isRepeat {
			break
		}

		// Start executing the transaction
		env.state.StartRecord(tx.Hash(), common.Hash{}, 0)

		err, logs := env.commitTransaction(tx, bc)
		switch {
		case core.IsGasLimitErr(err):
			//			// Pop the current out-of-gas transaction without shifting in the next from the account
			//			glog.V(logger.Detail).Infof("Gas limit reached for (%x) in this block. Continue to try smaller txs\n", from[:4])
			//			txs.Pop()

		case err != nil:
			// Pop the current failed transaction without shifting in the next from the account
			glog.V(logger.Detail).Infof("Transaction (%x) failed, will be removed: %v\n", tx.Hash().Bytes()[:4], err)
			env.failedTxs = append(env.failedTxs, tx)
			txs.Pop()

		default:
			// Everything ok, collect the logs and shift in the next transaction from the same account
			coalescedLogs = append(coalescedLogs, logs...)
			env.tcount++
			txs.Shift()
		}
	}
	if len(coalescedLogs) > 0 || env.tcount > 0 {
		go func(logs vm.Logs, tcount int) {
			if len(logs) > 0 {
				mux.Post(core.PendingLogsEvent{Logs: logs})
			}
			if tcount > 0 {
				mux.Post(core.PendingStateEvent{})
			}
		}(coalescedLogs, env.tcount)
	}
}*/

/*
* åŸºæœ¬æ€è·¯ï¼šObjects_num+1ä¸ªåç¨‹ï¼Œæ¯ä¸ªWorkeråç¨‹è´Ÿè´£ä¸€ä¸ªobjectçš„äº¤æ˜“çš„æ‰§è¡Œï¼Œ
* 1ä¸ªä¸»åç¨‹ï¼ˆMasterï¼‰è´Ÿè´£æ”¶é›†æ‰€æœ‰workerçš„æ‰§è¡Œæƒ…å†µ
* workerå°†æœ¬åç¨‹å·²ç»å®Œæˆçš„è¢«å…¶ä»–workerä¾èµ–çš„äº¤æ˜“ï¼ˆDependency_TXDoneï¼‰å‘é€ç»™Masterï¼ŒMasterå†å°†å…¶è½¬å‘ç»™ç›¸åº”çš„worker
* workeræ”¶åˆ°Masterçš„Dependency_TXDoneæ¶ˆæ¯åï¼Œåˆ¤æ–­æ˜¯å¦æ‰€æœ‰ä¾èµ–éƒ½æ»¡è¶³ï¼Œè‹¥æ»¡è¶³åˆ™ç»§ç»­æ‰§è¡Œ
* å½“workeræ‰§è¡Œå®Œè‡ªå·±çš„æ‰€æœ‰äº¤æ˜“æ—¶ï¼Œå°†ObjectDoneæ¶ˆæ¯å‘é€ç»™Master
* Masteræ”¶åˆ°ObjectDoneæ¶ˆæ¯åï¼Œè¿›è¡Œwaitg.Done()ç­‰å·¥ä½œ
 */

const (
	Dependency_TXDone   = "Dependency_TXDone"
	TXDone              = "TXDone"
	ObjectDone          = "ObjectDone"
	TXFailed            = "TXFailed"
	Dependency_TXFailed = "Dependency_TXFailed"
)

type Msg struct {
	Type     string
	OID      common.Address
	TxID     int
	Logs     Logs
	Receipts map[int]*Receipt
	Receipt  *Receipt
	//Txs map[int]*TXDM.Transaction
}

//var Dependency_TX //FIXME æ‰€æœ‰çš„ä¾èµ–å…³ç³»åŸæ¥æ˜¯string/int// äº¤æ˜“ä¾èµ–ï¼Œtxid/txid/oidäº¤æ˜“è¢«é‚£äº›äº¤æ˜“ä¾èµ–,ç¬¬äºŒå±‚çš„valueè¡¨ç¤ºoid
// æ¯ä¸€ä¸ªObjectçš„æ‰§è¡Œåºåˆ—ï¼Œintè¡¨ç¤ºå¯¹è±¡IDï¼›
/*
func (env *Work) commitTransactions_parallel_1(mux *event.TypeMux, txs TXDM.Transactions, bc *core.BlockChain,
	Objects_ExeSeqs map[common.Address]core.Object_ExeSeq, Dependency_TX map[int]map[common.Address]int) {

	//stopper := profile.Start(profile.MemProfile, profile.ProfilePath("./committx"))
	//defer stopper.Stop()

	//å¦‚æœæ˜¯ç©ºåŒºå—ç›´æ¥è¿”å›
	if len(txs) == 0 {
		return
	}
	//FIXME ä¸»åç¨‹å…±äº«å˜é‡ï¼Œæ˜¯å¦éœ€è¦ä¼ é€’
	objects_num := len(Objects_ExeSeqs)          // è¦æ‰§è¡Œçš„objectæ•°é‡
	var coalescedLogs vm.Logs                    //logsæ”¶é›†
	receipts := make(map[int]*ReceiptDM.Receipt) //receiptsæ”¶é›†

	// æ•°æ®åˆå§‹åŒ–
	var WorkerChannels sync.Map     //make(map[common.Address]chan Msg) // Object_ExeSeqçš„channelï¼Œintè¡¨ç¤ºobjectid
	MasterChannel := make(chan Msg) // MasterChannel
	//è®°å½•è¢«å…³é—­çš„workerChannelï¼Œé¿å…masterchan ç»™å·²å…³é—­channelå‘æ¶ˆæ¯
	WorkerChanneClosed := make(map[common.Address]bool, len(Objects_ExeSeqs))
	//åˆå§‹åŒ– workerChannel
	for oid, _ := range Objects_ExeSeqs {
		WorkerChannels.Store(oid, make(chan Msg))
	}

	// è¿›ç¨‹åˆ’åˆ†
	var waitg sync.WaitGroup
	waitg.Add(objects_num + 1) // one master thread, objects_num worker thread

	glog.V(logger.Info).Infoln("--start-mastercthread, txlen:", len(txs))
	// master cothread
	go func(WorkerChannels sync.Map, MasterChannel chan Msg, txs TXDM.Transactions,
		Dependency_TX map[int]map[common.Address]int, waitg *sync.WaitGroup) {

		counter := 0 // å·²å®Œæˆçš„objectçš„è®¡æ•°å™¨

		for {
			select {
			case msg := <-MasterChannel:

				if msg.Type == Dependency_TXDone || msg.Type == Dependency_TXFailed {
					//if msg.Type == Dependency_TXFailed{
					//	glog.V(logger.Info).Infoln("=== DFailed",msg.TxID)
					//}
					if dobjs, ok := Dependency_TX[msg.TxID]; ok {
						//if msg.Type == Dependency_TXFailed{
						//	glog.V(logger.Info).Infoln("=== DFailed",msg.TxID)
						//}
						for oid, _ := range dobjs {
							//æœªå…³é—­åˆ™å‘é€ï¼Œæ­¤å¤„åŠ åˆ¤æ–­ï¼Œå› ä¸ºå­˜åœ¨failedåï¼Œæ•´ä¸ªåˆ†æ”¯éƒ½failedï¼Œå³ä½¿ä¾èµ–äº¤æ˜“æ²¡åˆ°ï¼Œä¹Ÿä¼šobjdoneï¼Œclosechan
							if !WorkerChanneClosed[oid] {
								wch, _ := WorkerChannels.Load(oid) // è·å–æŒ‡å®šOIDçš„channel
								wch.(chan Msg) <- msg
							}

						}
					}
				}

				if msg.Type == ObjectDone {

					// æŸä¸ªobjectçš„æ‰€æœ‰äº¤æ˜“å·²å¤„ç†å®Œæˆ
					//glog.V(logger.Info).Infoln("--------master receive ObjectDone----------")

					for txid, receipt := range msg.Receipts {
						receipts[txid] = receipt
					}
					coalescedLogs = append(coalescedLogs, msg.Logs...)

					waitg.Done()
					//glog.V(logger.Info).Infoln("--------worker wait done----------", msg.OID.Hex())

					WorkerChanneClosed[msg.OID] = true
					wch, _ := WorkerChannels.Load(msg.OID) // è·å–æŒ‡å®šOIDçš„channel
					go close(wch.(chan Msg))

					counter++
					if counter == objects_num {
						waitg.Done() //æœ€åä¸€ä¸ª
						//glog.V(logger.Info).Infoln("--------master wait done----------")

						goto Loop
					}
				}
			}
		}
	Loop:
	}(WorkerChannels, MasterChannel, txs, Dependency_TX, &waitg)

	//glog.V(logger.Info).Infoln("--ctp-worker cothread")
	// worker cothread //FIXME txsæ˜¯å…±äº«å˜é‡
	for oid, oe := range Objects_ExeSeqs {
		wch, _ := WorkerChannels.Load(oid)
		go func(wch chan Msg, MasterChannel chan Msg, oid common.Address, oe core.Object_ExeSeq, txs TXDM.Transactions) {

			//è®°å½•è¯¥workeræ‰§è¡Œäº¤æ˜“çš„receipt keyäº¤æ˜“åºå·txidï¼Œvalueè¿”å›çš„receipt
			Receipts := make(map[int]*ReceiptDM.Receipt)
			var Log vm.Logs

			objTxLen := len(oe)

			execuing := -1 //æ­£åœ¨æ‰§è¡Œçš„txidï¼Œåˆå§‹åŒ–ä¸º-1
			// åˆ¤æ–­ç¬¬ä¸€ç¬”äº¤æ˜“æ˜¯å¦å¯æ‰§è¡Œ
			if len(oe[0].Dependency) == 0 {
				txid := oe[0].TX
				execuing++
				go env.executor_1(txid, txs[txid], bc, wch) // æ‰§è¡Œäº¤æ˜“execNum
			}

			//success := 0 //å·²æˆåŠŸçš„äº¤æ˜“è®¡æ•°å™¨
			counter := 0 // å·²å¤„ç†çš„äº¤æ˜“çš„è®¡æ•°å™¨
			for {
				select {
				case msg := <-wch:
					if msg.Type == TXDone || msg.Type == TXFailed {
						//æ‰§è¡ŒæˆåŠŸ
						// 0ã€è®°å½•ä¼ å›æ¥çš„å€¼
						// 1ã€counter++
						// 2ã€é€šçŸ¥ä¸»è¿›ç¨‹æœ‰ä¾èµ–
						// 3ã€æ ¹æ®counteråˆ¤æ–­æœ¬ç»„æ˜¯å¦æ‰§è¡Œå®Œæ¯•ï¼Œå®Œæ¯•å‘é€objectDoneæ¶ˆæ¯
						// 5ã€æ²¡æœ‰å®Œæ¯•ï¼Œåˆ™åˆ¤æ–­è‡ªå·±çš„ä¸‹ä¸€ä¸ªèƒ½å¦æ‰§è¡Œï¼Œèƒ½æ‰§è¡Œåˆ™å¼€å¯

						//glog.V(logger.Info).Infoln("--------receive TXDone----------", msg.TxID)

						//è®°å½•ä¼ å›æ¥çš„å€¼
						Receipts[msg.TxID] = msg.Receipt
						Log = append(Log, msg.Logs...)

						counter++
						if oe[counter-1].BeDependent { //å¦‚æœè¢«ä¾èµ–
							//glog.V(logger.Info).Infoln("--------BeDependent----------", msg.TxID)

							MasterChannel <- Msg{
								Type: "Dependency_" + msg.Type,
								TxID: msg.TxID,
							}
						}

						//glog.V(logger.Info).Infoln("--------",counter, txLen)

						if counter == objTxLen {
							//glog.V(logger.Info).Infoln("--------worker objDone----------", msg.TxID)

							MasterChannel <- Msg{Type: ObjectDone, OID: oid, Receipts: Receipts, Logs: Log}
							goto Loop
						}

						//å¼€å¯ä¸‹ä¸€ä¸ªæ‰§è¡Œ
						tx := oe[counter]
						if len(tx.Dependency) == 0 {
							//glog.V(logger.Info).Infoln("--------do next----------", tx.TX)

							txid := tx.TX
							execuing++
							go env.executor_1(txid, txs[txid], bc, wch) // æ‰§è¡Œäº¤æ˜“executing_Num + 1
						}
					}

					if msg.Type == Dependency_TXDone || msg.Type == Dependency_TXFailed {
						//ä¾èµ–äº¤æ˜“æ‰§è¡Œå®Œ
						// 1ã€éå†è¯¥objç»„ï¼Œç§»é™¤å¯¹åº”çš„ä¾èµ–äº¤æ˜“
						// 2ã€æŸ¥çœ‹ä¸‹ä¸€ä¸ªäº¤æ˜“æ˜¯å¦è¿˜æœ‰ä¾èµ–äº¤æ˜“ï¼Œæ²¡æœ‰åˆ™å¼€å¯ä¸‹ä¸€ä¸ªæ‰§è¡Œ

						//glog.V(logger.Info).Infoln("--------worker Dependency_TXDone----------", msg.TxID)

						for _, tx := range oe {
							//if _, ok := Dependency_TX[msg.TxID][tx.TX]; ok {
							delete(tx.Dependency, msg.TxID)
							//break// FIXME åº”è¯¥ä»success å¼€å§‹ åº”è¯¥åˆ é™¤ä¸€ä¸ªå°±å¥½ä¸€ä¸ª ç›®å‰ä¸èƒ½ä¿è¯åé¢æ²¡è®¾ç½®è¯¥ä¾èµ–
							//}
						}

						if counter != execuing { //è¡¨ç¤ºä¸‹ä¸€ç¬”äº¤æ˜“æœªå¼€å§‹æ‰§è¡Œ
							if len(oe[counter].Dependency) == 0 {
								txid := oe[counter].TX
								execuing++
								go env.executor_1(txid, txs[txid], bc, wch) // æ‰§è¡Œäº¤æ˜“success
							}
						}
					}
				}
			}
		Loop:
		}(wch.(chan Msg), MasterChannel, oid, oe, txs)
	}
	waitg.Wait() // ç­‰å¾…æ‰€æœ‰çš„å®Œæˆ
	go close(MasterChannel)
	glog.V(logger.Info).Infoln("--end-worker cothread")
	//æ‰§è¡Œenv
	for txid, tx := range txs {
		if receipt, ok := receipts[txid]; ok {
			env.txs = append(env.txs, tx)
			env.receipts = append(env.receipts, receipt)
		} else {
			glog.V(logger.Error).Infoln("execute err", tx.Hash().Hex())
		}
	}

	env.tcount = env.tcount + len(receipts)

	//glog.V(logger.Info).Infoln("--ctp waitg.Wait() after----------")
	//time.Sleep(time.Second)
	if len(coalescedLogs) > 0 || env.tcount > 0 {
		go func(logs vm.Logs, tcount int) {
			if len(logs) > 0 {
				mux.Post(core.PendingLogsEvent{Logs: logs})
			}
			if tcount > 0 {
				mux.Post(core.PendingStateEvent{})
			}
		}(coalescedLogs, env.tcount)
	}
}*/

//æ”¹ï¼šæ³¨é‡Šäº†
//func (env *Work) executor_1(txid int, tx *TXDM.Transaction, bc *core.BlockChain, wch chan Msg) {
//	//glog.V(logger.Info).Infoln("--------txid", txid, tx.Object().Hex())
//	//FIXME ç›®å‰çš„æ¨¡å¼ï¼Œä¸‹é¢çš„é”™è¯¯åº”è¯¥ä¸ä¼šå‘ç”Ÿ
//	//äº¤æ˜“ä¸åº”è¯¥ä¸ºnilï¼Œå¦‚æœä¸ºnilï¼Œè‡ªå·±panic
//	//äº¤æ˜“ä¸åº”è¯¥é‡å¤ï¼Œé‡å¤åˆ™ç³»ç»Ÿé€»è¾‘æœ‰é—®é¢˜ç›´æ¥panic
//	for _, ele := range env.txs {
//		if tx.Hash() == ele.Hash() {
//			panic("tx is repeat")
//		}
//	}
//	// Start executing the transaction
//	env.state.StartRecord(tx.Hash(), common.Hash{}, 0)
//	//==========================================================
//	//	err, logs := env.commitTransaction(tx, bc)
//
//	// FIXME è™šæ‹Ÿæœºç”¨ï¼Œæš‚æ—¶æ³¨é‡Š
//	// this is a bit of a hack to force jit for the miners
//	config := env.config.VmConfig
//	//if !(config.EnableJit && config.ForceJit) {
//	//	config.EnableJit = false
//	//}
//	//config.ForceJit = false // disable forcing jit
//
//	receipt, logs, err := core.ApplyTransaction(env.config, bc, env.state, env.header, tx /*env.header.GasUsed*/, config)
//
//	if err != nil {
//		//åˆ é™¤statedb.msä¸­æ‰€æœ‰tx.hash ç‰ˆæœ¬çš„å¯¹è±¡ï¼Œå¹¶å°†è¯¥å¯¹è±¡çš„æœ€æ–°çŠ¶æ€ç½®ä¸ºpreç‰ˆæœ¬
//		//env.state.RevertDbVersion(tx.Hash())
//		// FIXME å¦‚æœå¤±è´¥äº†ï¼Œæ˜¯ä¸æ˜¯ä¸éœ€è¦å›æ»šäº†
//		// 1ã€å¦‚æœå›æ»šï¼Œé…é¢æ‰£å‡ä¼šä¸æˆåŠŸ
//		// 2ã€å¦‚æœå‡ºé”™æ˜¯åœ¨é…é¢æ‰£å‡ä¹‹å‰ï¼Œä¼španicï¼Œä¸ä¼šèµ°åˆ°è¿™é‡Œ
//		// 3ã€å¦‚æœerr æ˜¯åœ¨é…é¢æ‰£å‡ä¹‹åï¼Œå¦‚æœæ˜¯æ£€éªŒé”™è¯¯ï¼Œä¸éœ€è¦å›æ»š
//		// 4ã€å¦‚æœæ˜¯æ‰§è¡Œé”™è¯¯ï¼Œä¸ä¼šæ›´æ–°statedbï¼Œä¸éœ€è¦å›æ»šã€å› ä¸ºæ‰§è¡Œå•å…ƒä¸­ï¼Œæ²¡æœ‰å¯¹fromçš„ptokenæ‰§è¡Œã€‘
//	}
//
//	//master æ‰§è¡Œ
//	//env.txs = append(env.txs, tx)
//	//env.receipts = append(env.receipts, receipt)
//
//	//===============================================
//
//	switch {
//	case core.IsGasLimitErr(err):
//		//			// Pop the current out-of-gas transaction without shifting in the next from the account
//		//			glog.V(logger.Detail).Infof("Gas limit reached for (%x) in this block. Continue to try smaller txs\n", from[:4])
//		//			txs.Pop()
//		glog.V(logger.Info).Infoln("======== 0 ===", txid, err) //å…±è¯†ä¸»èŠ‚ç‚¹è°ƒè¯•
//
//		wch <- Msg{TxID: txid, Type: TXFailed, Receipt: receipt, Logs: logs}
//
//		return
//	case err != nil:
//		// Pop the current failed transaction without shifting in the next from the account
//		glog.V(logger.Detail).Infof("Transaction (%x) failed, will be removed: %v\n", tx.Hash().Bytes()[:4], err)
//
//		//env.failedTxs = append(env.failedTxs, tx) //masteræ‰§è¡Œ
//		glog.V(logger.Info).Infoln("======== 0 ===", txid, err) //å…±è¯†ä¸»èŠ‚ç‚¹è°ƒè¯•
//
//		wch <- Msg{TxID: txid, Type: TXFailed, Receipt: receipt, Logs: logs}
//
//		//glog.V(logger.Info).Infoln("-----txfailed----------")
//		return
//
//	default:
//		// Everything ok, collect the logs and shift in the next transaction from the same account
//
//		//masteræ‰§è¡Œ
//		//coalescedLogs = append(coalescedLogs, logs...)
//		//env.tcount++
//		//glog.V(logger.Info).Infoln("--------executor TXDone----------", txid)
//		glog.V(logger.Info).Infoln("======== 1 ===", txid) //å…±è¯†ä¸»èŠ‚ç‚¹è°ƒè¯•
//
//		wch <- Msg{TxID: txid, Type: TXDone, Receipt: receipt, Logs: logs}
//
//		return
//	}
//}

type TXDoneMsg struct {
	//Type     string
	//OID      common.Address
	Err  error
	TxID int
	Logs Logs
	//Receipts map[int]*ReceiptDM.Receipt
	Receipt *Receipt
	//Txs map[int]*TXDM.Transaction
} //workerè‡ªå·±å‘ç»™worker

type ObjectDoneMsg struct {
	//Type     string
	OID string
	//TxID     int
	Logs     Logs
	Receipts map[int]*Receipt
	//Receipt  *ReceiptDM.Receipt
	//Txs map[int]*TXDM.Transaction
} //workerå‘ç»™master

type DependencyDoneMsg struct {
	//Type     string
	//OID      common.Address
	TxID int
	//Logs     vm.Logs
	//Receipts map[int]*ReceiptDM.Receipt
	//Receipt  *ReceiptDM.Receipt
	//Txs map[int]*TXDM.Transaction
} //workerå‘ç»™masterï¼Œmasterå‘ç»™worker

//var Dependency_TX //FIXME æ‰€æœ‰çš„ä¾èµ–å…³ç³»åŸæ¥æ˜¯string/int// äº¤æ˜“ä¾èµ–ï¼Œtxid/txid/oidäº¤æ˜“è¢«é‚£äº›äº¤æ˜“ä¾èµ–,ç¬¬äºŒå±‚çš„valueè¡¨ç¤ºoid
// æ¯ä¸€ä¸ªObjectçš„æ‰§è¡Œåºåˆ—ï¼Œintè¡¨ç¤ºå¯¹è±¡IDï¼›

//æ”¹ï¼šéªŒè¯å–å¾—çš„äº¤æ˜“æ˜¯å¦åˆæ³•
func (env *Work) commitTransactions_parallel(mux *event.TypeMux, /* txs TXDM.Transactions, bc *core.BlockChain,
	Objects_ExeSeqs map[string]core.Object_ExeSeq, Dependency_TX map[int]map[string]int*/txs string, bc *BlockChain,
	Objects_ExeSeqs [50]int, Dependency_TX [50]int) bool {

	//æ”¹ï¼š
	a := mux
	b := txs
	c := bc
	d := Objects_ExeSeqs
	e := Dependency_TX
	fmt.Println(a, b, c, d, e)
	//äººå·¥ä¿®æ”¹äº¤æ˜“æ˜¯å¦åˆæ³•
	return true

	//stopper := profile.Start(profile.MemProfile, profile.ProfilePath("./committx"))
	//defer stopper.Stop()

	//å¦‚æœæ˜¯ç©ºåŒºå—ç›´æ¥è¿”å›
	//if len(txs) == 0 {
	//	return
	//}
	////FIXME ä¸»åç¨‹å…±äº«å˜é‡ï¼Œæ˜¯å¦éœ€è¦ä¼ é€’
	//objects_num := len(Objects_ExeSeqs)          // è¦æ‰§è¡Œçš„objectæ•°é‡
	//var coalescedLogs vm.Logs                    //logsæ”¶é›†
	//receipts := make(map[int]*ReceiptDM.Receipt) //receiptsæ”¶é›†
	//
	//// æ•°æ®åˆå§‹åŒ–
	//MasterMux := new(event.TypeMux) // MasterChannel
	//MasterEvent := MasterMux.Subscribe(ObjectDoneMsg{}, DependencyDoneMsg{})
	//
	//var WorkerMuxs sync.Map //make(map[common.Address]chan Msg) // Object_ExeSeqçš„channelï¼Œintè¡¨ç¤ºobjectid
	//var WorkerEvents sync.Map
	//
	////åˆå§‹åŒ– workerChannel
	//for oid, _ := range Objects_ExeSeqs {
	//	workerMux := new(event.TypeMux)
	//	WorkerMuxs.Store(oid, workerMux)
	//	workerEvent := workerMux.Subscribe(TXDoneMsg{}, DependencyDoneMsg{})
	//	WorkerEvents.Store(oid, workerEvent)
	//}
	//
	//// è¿›ç¨‹åˆ’åˆ†
	//var waitg sync.WaitGroup
	//waitg.Add(objects_num + 1) // one master thread, objects_num worker thread
	//
	//glog.V(logger.Info).Infoln("--start-mastercthread, txlen:", len(txs))
	//// master cothread
	//go func(mevent event.Subscription, workerMux, wokerEvents sync.Map, txs TXDM.Transactions,
	//	Dependency_TX map[int]map[string]int, waitg *sync.WaitGroup) {
	//
	//	counter := 0 // å·²å®Œæˆçš„objectçš„è®¡æ•°å™¨
	//
	//	for ev := range mevent.Chan() {
	//		// A real event arrived, process interesting content
	//		//glog.V(logger.Info).Infoln("---m recv msg---", ev)
	//		switch msg := ev.Data.(type) {
	//		case DependencyDoneMsg:
	//			if dobjs, ok := Dependency_TX[msg.TxID]; ok {
	//				for oid, _ := range dobjs {
	//					//glog.V(logger.Info).Infoln("====m recv depend", msg.TxID, "bedepend----", oid.Hex())
	//					temp, _ := workerMux.Load(oid)
	//					wmux := temp.(*event.TypeMux)
	//					go wmux.Post(msg)
	//					//glog.V(logger.Info).Infoln("====m after send depend", msg.TxID, "----", oid.Hex())
	//				}
	//			} else {
	//				//glog.V(logger.Info).Infoln("====m recv depend", msg.TxID, "not bedepend")
	//			}
	//
	//		case ObjectDoneMsg:
	//
	//			// æŸä¸ªobjectçš„æ‰€æœ‰äº¤æ˜“å·²å¤„ç†å®Œæˆ
	//			//glog.V(logger.Info).Infoln("====m recv objdone====", msg.OID.Hex())
	//
	//			for txid, receipt := range msg.Receipts {
	//				receipts[txid] = receipt
	//			}
	//			coalescedLogs = append(coalescedLogs, msg.Logs...)
	//
	//			waitg.Done()
	//
	//			counter++
	//			if counter == objects_num {
	//				waitg.Done() //æœ€åä¸€ä¸ª
	//				goto Loop
	//			}
	//
	//		default:
	//			glog.V(logger.Info).Infof("recv a unscripriton event %d", msg)
	//		}
	//	}
	//Loop:
	//}(MasterEvent, WorkerMuxs, WorkerEvents, txs, Dependency_TX, &waitg)
	//
	//// worker cothread //FIXME txsæ˜¯å…±äº«å˜é‡
	//for oid, oe := range Objects_ExeSeqs {
	//	wch, _ := WorkerMuxs.Load(oid)
	//	wevent, _ := WorkerEvents.Load(oid)
	//	go func(wmux, mmux *event.TypeMux, wevent event.Subscription, oid string, oe core.Object_ExeSeq, txs TXDM.Transactions) {
	//
	//		//è®°å½•è¯¥workeræ‰§è¡Œäº¤æ˜“çš„receipt keyäº¤æ˜“åºå·txidï¼Œvalueè¿”å›çš„receipt
	//		Receipts := make(map[int]*ReceiptDM.Receipt)
	//		var Log vm.Logs
	//
	//		objTxLen := len(oe)
	//
	//		execuing := -1 //æ­£åœ¨æ‰§è¡Œçš„txidï¼Œåˆå§‹åŒ–ä¸º-1
	//		// åˆ¤æ–­ç¬¬ä¸€ç¬”äº¤æ˜“æ˜¯å¦å¯æ‰§è¡Œ
	//		if len(oe[0].Dependency) == 0 {
	//			txid := oe[0].TX
	//			execuing++
	//			go env.executor(txid, txs[txid], bc, wmux) // æ‰§è¡Œäº¤æ˜“execNum
	//		}
	//
	//		counter := 0 // å·²å¤„ç†çš„äº¤æ˜“çš„è®¡æ•°å™¨
	//		for ev := range wevent.Chan() {
	//			// A real event arrived, process interesting content
	//
	//			switch msg := ev.Data.(type) {
	//			case TXDoneMsg:
	//				//æ‰§è¡ŒæˆåŠŸ
	//				// 0ã€è®°å½•ä¼ å›æ¥çš„å€¼
	//				// 1ã€counter++
	//				// 2ã€é€šçŸ¥ä¸»è¿›ç¨‹æœ‰ä¾èµ–
	//				// 3ã€æ ¹æ®counteråˆ¤æ–­æœ¬ç»„æ˜¯å¦æ‰§è¡Œå®Œæ¯•ï¼Œå®Œæ¯•å‘é€objectDoneæ¶ˆæ¯
	//				// 5ã€æ²¡æœ‰å®Œæ¯•ï¼Œåˆ™åˆ¤æ–­è‡ªå·±çš„ä¸‹ä¸€ä¸ªèƒ½å¦æ‰§è¡Œï¼Œèƒ½æ‰§è¡Œåˆ™å¼€å¯
	//
	//				if msg.Err != DWVM.ErrMultiVersion { //å¦‚æœä¸æ˜¯å¤šç‰ˆæœ¬é”™è¯¯ï¼Œåˆ™å°å…¥åŒºå—ä¸­
	//					//è®°å½•ä¼ å›æ¥çš„å€¼
	//					Receipts[msg.TxID] = msg.Receipt
	//					Log = append(Log, msg.Logs...)
	//				}
	//
	//				counter++
	//				if oe[counter-1].BeDependent { //å¦‚æœè¢«ä¾èµ–
	//					//glog.V(logger.Info).Infoln("--------BeDependent----------", msg.TxID)
	//					//if counter == objTxLen{
	//					//	glog.V(logger.Info).Infoln("----------","end", "--------w recv txdone-----bedep-----", msg.TxID, oe[counter-1].BeDependent)
	//					//}else{
	//					//	glog.V(logger.Info).Infoln("----------",oe[counter].TX, "--------w recv txdone-----bedep-----", msg.TxID, oe[counter-1].BeDependent)
	//					//}
	//
	//					go mmux.Post(DependencyDoneMsg{msg.TxID})
	//					//if counter == objTxLen{
	//					//	glog.V(logger.Info).Infoln("----------","end", "--------w after send depend------", msg.TxID, oe[counter-1].BeDependent)
	//					//}else{
	//					//	glog.V(logger.Info).Infoln("----------",oe[counter].TX, "--------w after send depend------", msg.TxID, oe[counter-1].BeDependent)
	//					//}
	//				} else {
	//					//if counter == objTxLen{
	//					//	glog.V(logger.Info).Infoln("----------","end", "--------w recv txdone----not bedep-----", msg.TxID, oe[counter-1].BeDependent)
	//					//}else{
	//					//	glog.V(logger.Info).Infoln("----------",oe[counter].TX, "--------w recv txdone-----not bedep-----", msg.TxID, oe[counter-1].BeDependent)
	//					//}
	//				}
	//
	//				if counter == objTxLen {
	//					go mmux.Post(ObjectDoneMsg{OID: oid, Receipts: Receipts, Logs: Log})
	//					goto Loop
	//				}
	//
	//				//å¼€å¯ä¸‹ä¸€ä¸ªæ‰§è¡Œ
	//				tx := oe[counter]
	//				if len(tx.Dependency) == 0 {
	//					//glog.V(logger.Info).Infoln("--------do next----------", tx.TX)
	//
	//					txid := tx.TX
	//					execuing++
	//					go env.executor(txid, txs[txid], bc, wmux) // æ‰§è¡Œäº¤æ˜“executing_Num + 1
	//				}
	//
	//			case DependencyDoneMsg:
	//				//ä¾èµ–äº¤æ˜“æ‰§è¡Œå®Œ
	//				// 1ã€éå†è¯¥objç»„ï¼Œç§»é™¤å¯¹åº”çš„ä¾èµ–äº¤æ˜“
	//				// 2ã€æŸ¥çœ‹ä¸‹ä¸€ä¸ªäº¤æ˜“æ˜¯å¦è¿˜æœ‰ä¾èµ–äº¤æ˜“ï¼Œæ²¡æœ‰åˆ™å¼€å¯ä¸‹ä¸€ä¸ªæ‰§è¡Œ
	//
	//				//if counter == objTxLen{
	//				//	glog.V(logger.Info).Infoln("-------------","end", "--------w recv depend----------", msg.TxID)
	//				//}else{
	//				//	glog.V(logger.Info).Infoln("-------------",oe[counter].TX, "--------w recv depend----------", msg.TxID)
	//				//}
	//				for _, tx := range oe {
	//					//if _, ok := Dependency_TX[msg.TxID][tx.TX]; ok {
	//					delete(tx.Dependency, msg.TxID)
	//					//break// FIXME åº”è¯¥ä»success å¼€å§‹ åº”è¯¥åˆ é™¤ä¸€ä¸ªå°±å¥½ä¸€ä¸ª ç›®å‰ä¸èƒ½ä¿è¯åé¢æ²¡è®¾ç½®è¯¥ä¾èµ–
	//					//}
	//				}
	//
	//				if counter != execuing { //è¡¨ç¤ºä¸‹ä¸€ç¬”äº¤æ˜“æœªå¼€å§‹æ‰§è¡Œ
	//					if len(oe[counter].Dependency) == 0 {
	//						txid := oe[counter].TX
	//						execuing++
	//						go env.executor(txid, txs[txid], bc, wmux) // æ‰§è¡Œäº¤æ˜“success
	//					}
	//				}
	//
	//			default:
	//				glog.V(logger.Info).Infof("recv a unscripriton event %d", msg)
	//			}
	//		}
	//	Loop:
	//	}(wch.(*event.TypeMux), MasterMux, wevent.(event.Subscription), oid, oe, txs)
	//}
	//waitg.Wait() // ç­‰å¾…æ‰€æœ‰çš„å®Œæˆ
	//
	//WorkerEvents.Range(func(k, v interface{}) bool {
	//	wev := v.(event.Subscription)
	//	wev.Unsubscribe()
	//	return true
	//})
	//
	//WorkerMuxs.Range(func(k, v interface{}) bool {
	//	wmux := v.(*event.TypeMux)
	//	wmux.Stop()
	//	return true
	//})
	//
	//MasterEvent.Unsubscribe()
	//MasterMux.Stop()
	//
	//glog.V(logger.Info).Infoln("--end-worker cothread")
	////æ‰§è¡Œenv
	//count := 0
	//for txid, tx := range txs {
	//	if receipt, ok := receipts[txid]; ok {
	//		env.txs = append(env.txs, tx)
	//		env.receipts = append(env.receipts, receipt)
	//	} else {
	//		count++
	//		//glog.V(logger.Error).Infoln("execute err", tx.Hash().Hex())
	//	}
	//}
	//if count != 0 {
	//	glog.V(logger.Error).Infoln("execute err txcount", count)
	//}
	//
	//env.tcount = env.tcount + len(receipts)
	//
	//if len(coalescedLogs) > 0 || env.tcount > 0 {
	//	go func(logs vm.Logs, tcount int) {
	//		if len(logs) > 0 {
	//			go mux.Post(core.PendingLogsEvent{Logs: logs})
	//		}
	//		if tcount > 0 {
	//			go mux.Post(core.PendingStateEvent{})
	//		}
	//	}(coalescedLogs, env.tcount)
	//}

}

//
//func (env *Work) executor(txid int, tx *TXDM.Transaction, bc *core.BlockChain, wch *event.TypeMux) {
//
//	//glog.V(logger.Info).Infoln("-------------------start", txid)
//	//FIXME ç›®å‰çš„æ¨¡å¼ï¼Œä¸‹é¢çš„é”™è¯¯åº”è¯¥ä¸ä¼šå‘ç”Ÿ
//	//äº¤æ˜“ä¸åº”è¯¥ä¸ºnilï¼Œå¦‚æœä¸ºnilï¼Œè‡ªå·±panic
//	//äº¤æ˜“ä¸åº”è¯¥é‡å¤ï¼Œé‡å¤åˆ™ç³»ç»Ÿé€»è¾‘æœ‰é—®é¢˜ç›´æ¥panic
//	for _, ele := range env.txs {
//		if tx.Hash() == ele.Hash() {
//			panic("tx is repeat")
//		}
//	}
//	// Start executing the transaction
//	env.state.StartRecord(tx.Hash(), common.Hash{}, 0)
//	//==========================================================
//	//	err, logs := env.commitTransaction(tx, bc)
//
//	// FIXME è™šæ‹Ÿæœºç”¨ï¼Œæš‚æ—¶æ³¨é‡Š
//	// this is a bit of a hack to force jit for the miners
//	config := env.config.VmConfig
//	//if !(config.EnableJit && config.ForceJit) {
//	//	config.EnableJit = false
//	//}
//	//config.ForceJit = false // disable forcing jit
//
//	receipt, logs, err := core.ApplyTransaction(env.config, bc, env.state, env.header, tx /*env.header.GasUsed*/, config)
//
//	if err != nil {
//		//åˆ é™¤statedb.msä¸­æ‰€æœ‰tx.hash ç‰ˆæœ¬çš„å¯¹è±¡ï¼Œå¹¶å°†è¯¥å¯¹è±¡çš„æœ€æ–°çŠ¶æ€ç½®ä¸ºpreç‰ˆæœ¬
//		//env.state.RevertDbVersion(tx.Hash())
//		// FIXME å¦‚æœå¤±è´¥äº†ï¼Œæ˜¯ä¸æ˜¯ä¸éœ€è¦å›æ»šäº†
//		// 1ã€å¦‚æœå›æ»šï¼Œé…é¢æ‰£å‡ä¼šä¸æˆåŠŸ
//		// 2ã€å¦‚æœå‡ºé”™æ˜¯åœ¨é…é¢æ‰£å‡ä¹‹å‰ï¼Œä¼španicï¼Œä¸ä¼šèµ°åˆ°è¿™é‡Œ
//		// 3ã€å¦‚æœerr æ˜¯åœ¨é…é¢æ‰£å‡ä¹‹åï¼Œå¦‚æœæ˜¯æ£€éªŒé”™è¯¯ï¼Œä¸éœ€è¦å›æ»š
//		// 4ã€å¦‚æœæ˜¯æ‰§è¡Œé”™è¯¯ï¼Œä¸ä¼šæ›´æ–°statedbï¼Œä¸éœ€è¦å›æ»šã€å› ä¸ºæ‰§è¡Œå•å…ƒä¸­ï¼Œæ²¡æœ‰å¯¹fromçš„ptokenæ‰§è¡Œã€‘
//	}
//
//	//master æ‰§è¡Œ
//	//env.txs = append(env.txs, tx)
//	//env.receipts = append(env.receipts, receipt)
//
//	//===============================================
//
//	switch {
//	case core.IsGasLimitErr(err):
//		//			// Pop the current out-of-gas transaction without shifting in the next from the account
//		//			glog.V(logger.Detail).Infof("Gas limit reached for (%x) in this block. Continue to try smaller txs\n", from[:4])
//		//			txs.Pop()
//		glog.V(logger.Info).Infoln("======== 0 ===", txid, err) //å…±è¯†ä¸»èŠ‚ç‚¹è°ƒè¯•
//
//		go wch.Post(TXDoneMsg{TxID: txid, Receipt: receipt, Logs: logs, Err: err})
//
//		return
//	case err != nil:
//		// Pop the current failed transaction without shifting in the next from the account
//		glog.V(logger.Detail).Infof("Transaction (%x) failed, will be removed: %v\n", tx.Hash().Bytes()[:4], err)
//
//		//env.failedTxs = append(env.failedTxs, tx) //masteræ‰§è¡Œ
//		if err != DWVM.ErrMultiVersion {
//			glog.V(logger.Info).Infoln("======== 0 ===", txid, err) //å…±è¯†ä¸»èŠ‚ç‚¹è°ƒè¯•
//		}
//		go wch.Post(TXDoneMsg{TxID: txid, Receipt: receipt, Logs: logs, Err: err})
//		return
//
//	default:
//		// Everything ok, collect the logs and shift in the next transaction from the same account
//
//		//masteræ‰§è¡Œ
//		//coalescedLogs = append(coalescedLogs, logs...)
//		//env.tcount++
//		//glog.V(logger.Info).Infoln("======== 1 ===", txid)//å…±è¯†ä¸»èŠ‚ç‚¹è°ƒè¯•
//
//		go wch.Post(TXDoneMsg{TxID: txid, Receipt: receipt, Logs: logs, Err: err})
//
//		return
//	}
//}

//func (env *Work) commitTransaction(tx *TXDM.Transaction, bc *core.BlockChain) (error, vm.Logs) {
//	//snap := env.state.Snapshot()
//
//	// this is a bit of a hack to force jit for the miners
//	config := env.config.VmConfig
//	if !(config.EnableJit && config.ForceJit) {
//		config.EnableJit = false
//	}
//	config.ForceJit = false // disable forcing jit
//
//	receipt, logs, err := core.ApplyTransaction(env.config, bc, env.state, env.header, tx /*env.header.GasUsed*/, config)
//	//if err != nil {
//	//	env.state.RevertToSnapshot(snap)
//	//	return err, nil
//	//}
//
//	if err != nil {
//		//åˆ é™¤statedb.msä¸­æ‰€æœ‰tx.hash ç‰ˆæœ¬çš„å¯¹è±¡ï¼Œå¹¶å°†è¯¥å¯¹è±¡çš„æœ€æ–°çŠ¶æ€ç½®ä¸ºpreç‰ˆæœ¬
//		env.state.RevertDbVersion(tx.Hash())
//		return err, nil
//	}
//	env.txs = append(env.txs, tx)
//	env.receipts = append(env.receipts, receipt)
//
//	return nil, logs
//}
//
//func accountAddressesSet(accounts []accounts.Account) *set.Set {
//	accountSet := set.New()
//	for _, account := range accounts {
//		accountSet.Add(account.Address)
//	}
//	return accountSet
//}

func (instance *worker) removeOldConMsg(reqID uint64) {
	instance.certMux.Lock()
	for msgId, _ := range instance.certStore {
		if msgId.n <= reqID {
			delete(instance.certStore, msgId)
		}
	}
	instance.certMux.Unlock()

	instance.reqStore.Range(func(k, v interface{}) bool {
		if v.(*Request).ReqID <= reqID {
			instance.reqStore.Delete(k)
		}
		return true
	})
}

//æ¸…ç©ºå…±è¯†çŠ¶æ€ï¼Œæ¢å¤å‚æ•°åˆ°ç¨‹åºåˆå§‹åŒ–çŠ¶æ€
func (self *worker) ResetConData() {
	self.conviewmux.Lock()
	self.view = self.chain.CurrentBlock //().NumberU64() / TermNum // view åˆå§‹åŒ–
	self.conviewmux.Unlock()
	self.ResetCommitStore()
	self.SetReady(false) // æœªé€‰ä¸¾
	self.SetActiveView(true)

	self.reqStore.Range(func(k, v interface{}) bool {
		self.reqStore.Delete(k)
		return true
	})
	self.ResetPVBAndBeats()
	self.certMux.Lock()
	for k, _ := range self.certStore {
		delete(self.certStore, k)
	}
	self.certMux.Unlock()
	self.checkpointStore.Range(func(k, v interface{}) bool {
		self.checkpointStore.Delete(k)
		return true
	})
	if consensusCancel != nil {
		consensusCancel()
	}

	//go self.CheckConStatus() // update constatus
}

//minerå†…éƒ¨è°ƒç”¨
func (self *worker) ResetConData2() {
	//self.view = self.chain.CurrentBlock().NumberU64() / TermNum // view åˆå§‹åŒ–
	//self.SetReady(false)     // æœªé€‰ä¸¾
	//self.SetActiveView(true)

	self.reqStore.Range(func(k, v interface{}) bool {
		self.reqStore.Delete(k)
		return true
	})
	self.ResetPVBAndBeats()
	self.certMux.Lock()
	for k, _ := range self.certStore {
		delete(self.certStore, k)
	}
	self.certMux.Unlock()
	self.checkpointStore.Range(func(k, v interface{}) bool {
		self.checkpointStore.Delete(k)
		return true
	})

	if consensusCancel != nil {
		consensusCancel()
	}

	//go self.CheckConStatus() // update constatus
}

func (self *worker) ForTestMem() {
}

func (self *worker) SetTxNum_startconsens(num int) {
	self.txNum_startconsens = num
}
